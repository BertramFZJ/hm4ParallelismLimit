#!/bin/bash
#SBATCH --job-name ./parLimit
#SBATCH --output ./console.out
#SBATCH --error ./console.err
#SBATCH --partition=compute  # Specify partition name for job execution
#SBATCH --ntasks-per-node=128
#SBATCH --exclusive
#SBATCH --mem=0
# ### PART 2: modify according to your requirements:
#SBATCH --nodes=1
#SBATCH --time=00:15:00
#SBATCH --account=ka1298
#SBATCH --mail-type=NONE

## The ompi_info tool can be used to get detailed information about
## OpenMPI installation and local configuration:
## ompi_info --all

export OMPI_MCA_osc="ucx"
export OMPI_MCA_pml="ucx"
export OMPI_MCA_btl="self"
export UCX_HANDLE_ERRORS="bt"
export OMPI_MCA_pml_ucx_opal_mem_hooks=1

export OMPI_MCA_io="romio321"          # basic optimisation of I/O
export UCX_TLS="shm,rc_mlx5,rc_x,self" # for jobs using LESS than 150 nodes
## export UCX_TLS="shm,dc_mlx5,dc_x,self" # for jobs using MORE than 150 nodes
export UCX_UNIFIED_MODE="y"            # JUST for homogeneous jobs on CPUs, do not use for GPU nodes

srun ./parallelLimit.exe

## For some applications (e.g. high-resolution version of ICON) a special MPI_Alltoallv 
## algorithm has to be used in case you notice a deadlock in Alltoallv.
## export OMPI_MCA_coll_tuned_use_dynamic_rules="true"
## export OMPI_MCA_coll_tuned_alltoallv_algorithm=2

## --------------------------------------------- INTEL ---------------------------------------------

## export I_MPI_PMI=pmi
## export I_MPI_PMI_LIBRARY=/usr/lib64/libpmi.so

## export OMPI_MCA_osc="ucx"
## export OMPI_MCA_pml="ucx"
## export OMPI_MCA_btl="self"
## export UCX_HANDLE_ERRORS="bt"
## export OMPI_MCA_pml_ucx_opal_mem_hooks=1

## srun ./parallelLimit.exe

## ml scorep/7.0-intel-2021.5.0
## ml intel-oneapi-mpi/2021.5.0-intel-2021.5.0
## ml intel-oneapi-mpi/2021.5.0-gcc-11.2.0
## ml intel-oneapi-compilers/2022.0.1-gcc-11.2.0
## ml scalasca/2.6-gcc-11.2.0

## --------------------------------------------- INTEL ---------------------------------------------

